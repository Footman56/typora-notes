# 一次性查 200万数据，mysql 会oom 吗？

不会 , mysql 是边读边发的

1. 扫描引擎层： 扫描数据
2. 写入缓冲区：塞进net_buffer 内存区域， 默认16KB
3. 触发发送：一旦net_buffer写满或者数据读完，就发送给客户端
4. 清空复用： 发送成功后，清空net_buffer 继续



不要一次扫描太多数据，会导致mysql buffer pool 被污染，buffer pool 存的是最热数据。根据LRU 算法（最近最少使用算法）会把热点数据挤出内存。

mysqldump 走的流式查询，配合InnoDB 的改进LRU 分离策略

InnoDB 读数据 → 先查 Buffer Pool  不在的话 → 从磁盘读入 → 放入 Buffer Pool

LRU 被分成两部分 ，young 、old 区， 热区 63%，冷区37%，通过innodb_old_blocks_pct字段控制

1. 插入策略：当从磁盘读入新页时放到“冷区头部”
2. 如果一个页在冷区且访问间隔大于1s再次被访问升级到热区   通过innodb_old_blocks_time 默认1000ms

#  慢sql怎么优化

## 全链路监控

1. 打开慢查询日志
2. 实时监控  ，利用Promentheus + grafana    

## EXPLAIN

1. 检查type字段是否为 ref、range;  警惕 index 、all 
2. Extra   Using index ：完美覆盖索引 

## SELECT * FROM sys.statement_analysis;

1. 最耗时sql   total_latency
2. 执行次数最多 
3. 锁等待最多的

## trace

线上不要长期开启

```
SELECT * 
FROM information_schema.OPTIMIZER_TRACE;
```

##  架构

1. 适当采用e s, redis 或者报表类操作的话强制走从库



# 深度分页怎么优化

为什么会慢？

```
根据索引排序  依次扫描 丢弃前 offset 行  返回 limit 行
IO 高：扫描大量索引页； CPU高 丢弃大量记录 ；回表多：如果 select * ； 锁持有时间长 影响并发
```

## 基于最后一条记录分页

1. 不要调页，要下一页。要记录上一次分页最后的记录 

## 延迟关联

```
# 子查询只扫描索引,外层只回表 20 条
SELECT o.*
FROM order_info o
JOIN (
    SELECT id
    FROM order_info
    ORDER BY create_time DESC
    LIMIT 1000000, 20
) t ON o.id = t.id;
```

## 覆盖索引，不回表

返回的字段，排序的字段尽量都在索引里面，如果不在的话，可以考虑创建联合

## 业务层缓存

1. 限制最多页数

## es 代替深度分页

