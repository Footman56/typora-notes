系统架构如下：

<img src="https://raw.githubusercontent.com/Footman56/images-2/master/img202511291553691.png" alt="88bb3303-9b5c-47aa-8b39-4ae9ffbd489d" style="zoom:50%;" />

# 读取文件内容

```python
import pandas as pd

# Kaggle 数据路径
ch_path = "/kaggle/input/machine-translation-chinese-and-english/chinese.zh"
en_path = "/kaggle/input/machine-translation-chinese-and-english/english.en"

# 读取文件
with open(ch_path, "r", encoding="utf8") as f:
    chinese = f.read().strip().split("\n")

with open(en_path, "r", encoding="utf8") as f:
    english = f.read().strip().split("\n")

print("Chinese samples:", len(chinese))
print("English samples:", len(english))

# 确保长度相同
assert len(chinese) == len(english)
```



# 建立中文与英文的绑定

```python
import random

N = 1000
# 设置随机数种子，相同种子能保证每次运行时获取相同的数据
random.seed(42)

# zip 会把两个列表按顺序一一配对 chinese[0] 对应 english[0]
pairs = list(zip(chinese, english))
# 随机抽取1000个元素，每个元素是(chinese,english)
pairs = random.sample(pairs, N)
# pairs 是列表, *pairs 叫做“解包”，可以把列表中的元组重新按列拆开,将一个列表拆分成两个列表，并且两个列表相同下标的数据是一一对应的
chinese, english = zip(*pairs)
```



# 训练分词

词表就是一个“词 → 数字ID”的映射表。

词表的作用

+ 把文本转换成模型能理解的数字 【单词或子词 ：对应的数字 ID】
+ 限定模型能够处理的词的范围 【词表的大小】
+ 作为 Transformer 最后输出层的“词分类器” 

```python
import sentencepiece as spm
import os

os.makedirs("spm", exist_ok=True)

# 将文本保存以训练分词器，SentencePiece 需要文本文件作为输入，以训练专属分词模型。
with open("spm/ch.txt", "w", encoding="utf8") as f:
    for t in chinese:
        # 每条中文句子独占一行是推荐格式。
        f.write(t + "\n")

with open("spm/en.txt", "w", encoding="utf8") as f:
    for t in english:
        f.write(t + "\n")

# 训练中文分词器
'''
input: 输入的文本文件
model_prefix： 模型前缀，会生成两个文件
    spm/ch.model:分词模型
    spm/ch.vocab:词表（词 → id）
vocab_size:词表大小
model_type: BPE（Byte Pair Encoding）分词算法
character_coverage: 对中文非常重要 → 需要覆盖大部分汉字。覆盖 99.95%
'''
spm.SentencePieceTrainer.Train(
    input="spm/ch.txt",
    model_prefix="spm/ch",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=0.9995
)

'''
character_coverage: 英文单词少，可以全部覆盖
'''

# 训练英文分词器
spm.SentencePieceTrainer.Train(
    input="spm/en.txt",
    model_prefix="spm/en",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=1.0
)

sp_ch = spm.SentencePieceProcessor(model_file="spm/ch.model")
sp_en = spm.SentencePieceProcessor(model_file="spm/en.model")

# 打印词汇表数量，可以用于检查分词是否成功
print("Chinese vocab:", sp_ch.get_piece_size())
print("English vocab:", sp_en.get_piece_size())
```



# 将文本借助词汇表转换成张量

```python
import torch

PAD_ID = 0
# 代表句子开始
BOS_ID = 1
# 代表句子结束
EOS_ID = 2

# 设置句子最大长度，因为Transformer训练输入必须是固定长度矩阵，用于控制矩阵的大小
MAX_LEN = 40


# 将句子转换成列表形式，便于Transformer 识别，如果句子过短需要填充PAD_ID来保证长度为MAX_LEN
def encode(text, sp):
    ids = [BOS_ID] + sp.encode(text) + [EOS_ID]
    return ids[:MAX_LEN] + [PAD_ID] * (MAX_LEN - len(ids))

# 转换成张量 [[1,2,3...MAX_LEN],[]] 一行就是一个句子
src_data = torch.tensor([encode(t, sp_ch) for t in chinese])
tgt_data = torch.tensor([encode(t, sp_en) for t in english])

src_data.shape, tgt_data.shape
```

# 计算位置编码

为什么需要位置编码？

Transformer **没有 RNN 或 CNN**，它完全依赖注意力机制，Self-attention 只知道“词之间的关系”，但是不知道“第几个词”，我们必须告诉模型：每个 token 在句子中的位置

<img src="https://raw.githubusercontent.com/Footman56/images-2/master/img202511292225370.png" alt="image-20251129222556179" style="zoom:50%;" />

```python
import math
import torch.nn as nn

class PositionalEncoding(nn.Module):
    """生成并添加正弦位置编码
        位置编码的作用是 每个 token 在句子中的位置
        d_model:每个词最终都会被转换成一个固定长度的向量,向量的长度
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model) # pe.size() = [max_len,d_model]
        position = torch.arange(0, max_len).unsqueeze(1)  # position.size()=[max_len,1]
        # 使用对数避免数值问题
        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        # 给偶数维填入 sin，给奇数维填入 cos
        pe[:, 0::2] = torch.sin(position * div)
        pe[:, 1::2] = torch.cos(position * div)
        self.pe = pe.unsqueeze(0) # pe.size() = [1,max_len,d_model]
        print(f'{self.pe}')

    def forward(self, x):
      	# 将预先计算好的位置编码（positional encoding）加到输入序列的 embedding 上，为每个 token 注入“位置信息”
        return x + self.pe[:, :x.size(1)]
```

#  自注意力函数

```python
def attention(Q, K, V, mask=None):
    # 计算注意力分数
    scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    # 归一化处理，所有向量的和为1，数值在0～1之间
    attn = torch.softmax(scores, dim=-1)
    return attn @ V, attn
```

#  多头注意力机制

```python
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    """自定义多头注意力"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.h = num_heads
        self.d_k = d_model // num_heads

        self.WQ = nn.Linear(d_model, d_model)
        self.WK = nn.Linear(d_model, d_model)
        self.WV = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        B = Q.size(0)
        Q = self.WQ(Q).view(B, -1, self.h, self.d_k).transpose(1, 2)
        K = self.WK(K).view(B, -1, self.h, self.d_k).transpose(1, 2)
        V = self.WV(V).view(B, -1, self.h, self.d_k).transpose(1, 2)

        out, _ = attention(Q, K, V, mask)
        out = out.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)

        return self.out(out)
```

# 前馈网络

```python
class FeedForward(nn.Module):
    """两层前馈网络"""
    def __init__(self, d_model, hidden):
        super().__init__()
        self.fc1 = nn.Linear(d_model, hidden)
        self.fc2 = nn.Linear(hidden, d_model)

    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

```

```
```

