系统架构如下：

<img src="https://raw.githubusercontent.com/Footman56/images-2/master/img202511291553691.png" alt="88bb3303-9b5c-47aa-8b39-4ae9ffbd489d" style="zoom:50%;" />

# 读取文件内容

```python
import pandas as pd

# Kaggle 数据路径
ch_path = "/kaggle/input/machine-translation-chinese-and-english/chinese.zh"
en_path = "/kaggle/input/machine-translation-chinese-and-english/english.en"

# 读取文件
with open(ch_path, "r", encoding="utf8") as f:
    chinese = f.read().strip().split("\n")

with open(en_path, "r", encoding="utf8") as f:
    english = f.read().strip().split("\n")

print("Chinese samples:", len(chinese))
print("English samples:", len(english))

# 确保长度相同
assert len(chinese) == len(english)
```



# 建立中文与英文的绑定

```python
import random

N = 1000
# 设置随机数种子，相同种子能保证每次运行时获取相同的数据
random.seed(42)

# zip 会把两个列表按顺序一一配对 chinese[0] 对应 english[0]
pairs = list(zip(chinese, english))
# 随机抽取1000个元素，每个元素是(chinese,english)
pairs = random.sample(pairs, N)
# pairs 是列表, *pairs 叫做“解包”，可以把列表中的元组重新按列拆开,将一个列表拆分成两个列表，并且两个列表相同下标的数据是一一对应的
chinese, english = zip(*pairs)
```



# 训练分词

词表就是一个“词 → 数字ID”的映射表。

词表的作用

+ 把文本转换成模型能理解的数字 【单词或子词 ：对应的数字 ID】
+ 限定模型能够处理的词的范围 【词表的大小】
+ 作为 Transformer 最后输出层的“词分类器” 

```python
import sentencepiece as spm
import os

os.makedirs("spm", exist_ok=True)

# 将文本保存以训练分词器，SentencePiece 需要文本文件作为输入，以训练专属分词模型。
with open("spm/ch.txt", "w", encoding="utf8") as f:
    for t in chinese:
        # 每条中文句子独占一行是推荐格式。
        f.write(t + "\n")

with open("spm/en.txt", "w", encoding="utf8") as f:
    for t in english:
        f.write(t + "\n")

# 训练中文分词器
'''
input: 输入的文本文件
model_prefix： 模型前缀，会生成两个文件
    spm/ch.model:分词模型
    spm/ch.vocab:词表（词 → id）
vocab_size:词表大小
model_type: BPE（Byte Pair Encoding）分词算法
character_coverage: 对中文非常重要 → 需要覆盖大部分汉字。覆盖 99.95%
'''
spm.SentencePieceTrainer.Train(
    input="spm/ch.txt",
    model_prefix="spm/ch",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=0.9995
)

'''
character_coverage: 英文单词少，可以全部覆盖
'''

# 训练英文分词器
spm.SentencePieceTrainer.Train(
    input="spm/en.txt",
    model_prefix="spm/en",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=1.0
)

sp_ch = spm.SentencePieceProcessor(model_file="spm/ch.model")
sp_en = spm.SentencePieceProcessor(model_file="spm/en.model")

# 打印词汇表数量，可以用于检查分词是否成功
print("Chinese vocab:", sp_ch.get_piece_size())
print("English vocab:", sp_en.get_piece_size())
```



# 将文本借助词汇表转换成张量

```python
import torch

PAD_ID = 0
# 代表句子开始
BOS_ID = 1
# 代表句子结束
EOS_ID = 2

# 设置句子最大长度，因为Transformer训练输入必须是固定长度矩阵，用于控制矩阵的大小
MAX_LEN = 40


# 将句子转换成列表形式，便于Transformer 识别，如果句子过短需要填充PAD_ID来保证长度为MAX_LEN
def encode(text, sp):
    ids = [BOS_ID] + sp.encode(text) + [EOS_ID]
    return ids[:MAX_LEN] + [PAD_ID] * (MAX_LEN - len(ids))

# 转换成张量 [[1,2,3...MAX_LEN],[]] 一行就是一个句子
src_data = torch.tensor([encode(t, sp_ch) for t in chinese])
tgt_data = torch.tensor([encode(t, sp_en) for t in english])

src_data.shape, tgt_data.shape
```

# 计算位置编码

为什么需要位置编码？

Transformer **没有 RNN 或 CNN**，它完全依赖注意力机制，Self-attention 只知道“词之间的关系”，但是不知道“第几个词”，我们必须告诉模型：每个 token 在句子中的位置。

<img src="https://raw.githubusercontent.com/Footman56/images-2/master/img202511292225370.png" alt="image-20251129222556179" style="zoom:50%;" />

```python
import math
import torch.nn as nn

class PositionalEncoding(nn.Module):
    """生成并添加正弦位置编码
        位置编码的作用是 每个 token 在句子中的位置
        d_model:每个词最终都会被转换成一个固定长度的向量,向量的长度
    """
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model) # pe.size() = [max_len,d_model]
        position = torch.arange(0, max_len).unsqueeze(1)  # position.size()=[max_len,1]
        # 使用对数避免数值问题
        div = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        # 给偶数维填入 sin，给奇数维填入 cos
        pe[:, 0::2] = torch.sin(position * div)
        pe[:, 1::2] = torch.cos(position * div)
        self.pe = pe.unsqueeze(0) # pe.size() = [1,max_len,d_model]
        print(f'{self.pe}')

    def forward(self, x):
      	# 将预先计算好的位置编码（positional encoding）加到输入序列的 embedding 上，为每个 token 注入“位置信息”
        return x + self.pe[:, :x.size(1)]
```

#  自注意力函数

计算注意力函数时的输入 不是encode(text, sp) 生成的编码，而是对这个编码进行embedding 得出的向量 。

## embedding

Embedding 本质上是一个参数矩阵,

```python
# 就是创造一个  vocab_size *  d_model  的矩阵
nn.Embedding(vocab_size, d_model)
```

学习过程：

1. Embedding 初始化时是随机的。
2. 通过“反向传播 + 损失函数”，逐渐变成语义向量。



```python
def attention(Q, K, V, mask=None):
    # 计算注意力分数
    scores = Q @ K.transpose(-2, -1) / math.sqrt(Q.size(-1))
    if mask is not None:
        scores = scores.masked_fill(mask == 0, -1e9)
    # 归一化处理，所有向量的和为1，数值在0～1之间
    attn = torch.softmax(scores, dim=-1)
    return attn @ V, attn
```

#  多头注意力机制

```python
import torch.nn as nn

class MultiHeadAttention(nn.Module):
    """自定义多头注意力"""
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.h = num_heads
        self.d_k = d_model // num_heads

        self.WQ = nn.Linear(d_model, d_model)
        self.WK = nn.Linear(d_model, d_model)
        self.WV = nn.Linear(d_model, d_model)
        self.out = nn.Linear(d_model, d_model)

    def forward(self, Q, K, V, mask=None):
        B = Q.size(0)
        Q = self.WQ(Q).view(B, -1, self.h, self.d_k).transpose(1, 2)
        K = self.WK(K).view(B, -1, self.h, self.d_k).transpose(1, 2)
        V = self.WV(V).view(B, -1, self.h, self.d_k).transpose(1, 2)

        out, _ = attention(Q, K, V, mask)
        out = out.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)

        return self.out(out)
```

# 前馈网络

```python
class FeedForward(nn.Module):
    """两层前馈网络"""
    def __init__(self, d_model, hidden):
        super().__init__()
        self.fc1 = nn.Linear(d_model, hidden)
        self.fc2 = nn.Linear(hidden, d_model)

    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

```



#  编码层

```python
class EncoderLayer(nn.Module):
    def __init__(self, d_model, heads, hidden):
        super().__init__()
        self.attn = MultiHeadAttention(d_model, heads)
        self.ff = FeedForward(d_model, hidden)
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x, mask):
        x = self.norm1(x + self.attn(x, x, x, mask))
        x = self.norm2(x + self.ff(x))
        return x
```



#  解码层

```python
class DecoderLayer(nn.Module):
    def __init__(self, d_model, heads, hidden):
        super().__init__()
        self.self_attn = MultiHeadAttention(d_model, heads)
        self.cross_attn = MultiHeadAttention(d_model, heads)
        self.ff = FeedForward(d_model, hidden)

        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)

    def forward(self, x, enc_out, tgt_mask, src_mask):
        x = self.norm1(x + self.self_attn(x, x, x, tgt_mask))
        x = self.norm2(x + self.cross_attn(x, enc_out, enc_out, src_mask))
        x = self.norm3(x + self.ff(x))
        return x

```



# 编码&解码

包含多个编码层和解码层

```python
class Encoder(nn.Module):
    def __init__(self, vocab, d_model, heads, hidden, N):
        super().__init__()
        self.embed = nn.Embedding(vocab, d_model)
        self.pe = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([EncoderLayer(d_model, heads, hidden) for _ in range(N)])

    def forward(self, x, mask):
        x = self.pe(self.embed(x))
        for l in self.layers:
            x = l(x, mask)
        return x


class Decoder(nn.Module):
    def __init__(self, vocab, d_model, heads, hidden, N):
        super().__init__()
        self.embed = nn.Embedding(vocab, d_model)
        self.pe = PositionalEncoding(d_model)
        self.layers = nn.ModuleList([DecoderLayer(d_model, heads, hidden) for _ in range(N)])
        self.fc = nn.Linear(d_model, vocab)

    def forward(self, x, enc, tgt_mask, src_mask):
        x = self.pe(self.embed(x))
        for l in self.layers:
            x = l(x, enc, tgt_mask, src_mask)
        return self.fc(x)

```

# Transformer 整体

```python
class Transformer(nn.Module):
    def __init__(self, src_vocab, tgt_vocab, d_model=128, heads=4, hidden=256, N=2):
        super().__init__()
        self.encoder = Encoder(src_vocab, d_model, heads, hidden, N)
        self.decoder = Decoder(tgt_vocab, d_model, heads, hidden, N)

    def make_src_mask(self, src):
        return (src != PAD_ID).unsqueeze(1).unsqueeze(2)

    def make_tgt_mask(self, tgt):
        L = tgt.size(1)
        mask = torch.tril(torch.ones((L, L))).bool()
        return mask.unsqueeze(0).unsqueeze(1)

    def forward(self, src, tgt):
        src_mask = self.make_src_mask(src)
        tgt_mask = self.make_tgt_mask(tgt)
        enc = self.encoder(src, src_mask)
        out = self.decoder(tgt, enc, tgt_mask, src_mask)
        return out
```

# 训练

```python
import torch.optim as optim
from tqdm import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Transformer(
    src_vocab=sp_ch.get_piece_size(),
    tgt_vocab=sp_en.get_piece_size(),
).to(device)

criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)
optimizer = optim.Adam(model.parameters(), lr=1e-4)

for epoch in range(100):
    total = 0
    for i in tqdm(range(N)):
        src = src_data[i:i+1].to(device)
        tgt = tgt_data[i:i+1].to(device)

        dec_in = tgt[:, :-1]
        label = tgt[:, 1:]

        out = model(src, dec_in)
        loss = criterion(out.reshape(-1, out.size(-1)), label.reshape(-1))

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total += loss.item()

    print("loss:", total / N)
```

# 预测

```python
def translate(text):
    model.eval()
    src = torch.tensor([encode(text, sp_ch)]).to(device)
    ys = torch.tensor([[BOS_ID]]).to(device)

    for _ in range(MAX_LEN):
        out = model(src, ys)
        next_id = out[0, -1].argmax().item()
        ys = torch.cat([ys, torch.tensor([[next_id]]).to(device)], dim=1)
        if next_id == EOS_ID:
            break

    return sp_en.decode(ys[0].cpu().tolist()[1:-1])


print(translate("我喜欢机器学习"))
print(translate("你今天好吗"))

```

