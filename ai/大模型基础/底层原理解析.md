系统架构如下：

<img src="https://raw.githubusercontent.com/Footman56/images-2/master/img202511291553691.png" alt="88bb3303-9b5c-47aa-8b39-4ae9ffbd489d" style="zoom:50%;" />

1. 读取文件内容

```python
import pandas as pd

# Kaggle 数据路径
ch_path = "/kaggle/input/machine-translation-chinese-and-english/chinese.zh"
en_path = "/kaggle/input/machine-translation-chinese-and-english/english.en"

# 读取文件
with open(ch_path, "r", encoding="utf8") as f:
    chinese = f.read().strip().split("\n")

with open(en_path, "r", encoding="utf8") as f:
    english = f.read().strip().split("\n")

print("Chinese samples:", len(chinese))
print("English samples:", len(english))

# 确保长度相同
assert len(chinese) == len(english)
```



2. 建立中文与英文的绑定

```python
import random

N = 1000
# 设置随机数种子，相同种子能保证每次运行时获取相同的数据
random.seed(42)

# zip 会把两个列表按顺序一一配对 chinese[0] 对应 english[0]
pairs = list(zip(chinese, english))
# 随机抽取1000个元素，每个元素是(chinese,english)
pairs = random.sample(pairs, N)
# pairs 是列表, *pairs 叫做“解包”，可以把列表中的元组重新按列拆开,将一个列表拆分成两个列表，并且两个列表相同下标的数据是一一对应的
chinese, english = zip(*pairs)
```



3. 训练分词

   **词表就是一个“词 → 数字ID”的映射表。**

```python
import sentencepiece as spm
import os

os.makedirs("spm", exist_ok=True)

# 将文本保存以训练分词器，SentencePiece 需要文本文件作为输入，以训练专属分词模型。
with open("spm/ch.txt", "w", encoding="utf8") as f:
    for t in chinese:
        # 每条中文句子独占一行是推荐格式。
        f.write(t + "\n")

with open("spm/en.txt", "w", encoding="utf8") as f:
    for t in english:
        f.write(t + "\n")

# 训练中文分词器
'''
input: 输入的文本文件
model_prefix： 模型前缀，会生成两个文件
    spm/ch.model:分词模型
    spm/ch.vocab:词表（词 → id）
vocab_size:词表大小
model_type: BPE（Byte Pair Encoding）分词算法
character_coverage: 对中文非常重要 → 需要覆盖大部分汉字。覆盖 99.95%
'''
spm.SentencePieceTrainer.Train(
    input="spm/ch.txt",
    model_prefix="spm/ch",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=0.9995
)

'''
character_coverage: 英文单词少，可以全部覆盖
'''

# 训练英文分词器
spm.SentencePieceTrainer.Train(
    input="spm/en.txt",
    model_prefix="spm/en",
    vocab_size=2000,
    model_type="bpe",
    character_coverage=1.0
)

sp_ch = spm.SentencePieceProcessor(model_file="spm/ch.model")
sp_en = spm.SentencePieceProcessor(model_file="spm/en.model")

# 打印词汇表数量，可以用于检查分词是否成功
print("Chinese vocab:", sp_ch.get_piece_size())
print("English vocab:", sp_en.get_piece_size())
```



