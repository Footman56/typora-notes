1. 大语言模型与生成式模型的区别？

   大语言模型是生成式模型的子集。大语言模型侧重于文本的生成，生成式模型可以是多模态的输出（文本、图片、视频）。都基于Transformer框架

   | 维度     | 生成式模型                     | 大语言模型（LLM）                |
   | -------- | ------------------------------ | -------------------------------- |
   | 适用范围 | 多模态（文本/图像/音频/视频）  | 专注文本领域（少数扩展为多模态） |
   | 典型架构 | GAN/VAE/Diffusion/自回归       | 基于Transformer的自回归模型      |
   | 规模要求 | 部分模型可轻量化（如GAN）      | 必须超大规模（十亿级参数起步）   |
   | 训练数据 | 依赖领域数据（如CelebA数据集） | 需海量通用文本（如Common Crawl） |
   | 核心能力 | 生成逼真样本                   | 语言理解+逻辑推理+上下文生成     |
   | 典型应用 | AI绘画、语音合成               | 智能对话、代码生成、文本摘要     |

​		

​		大语言模型核心特点：

		+ 参数量大
		+ 通用性强：通过海量文本训练，可处理多种语言任务
		+ 上下文学习：支持长文本连贯生成（如对话、文章续写）

2. 课程里面的神经网络没有理解，需要看机器学习的内容吗？

```
课程学习时理解概念，在课后学习 机器学习&深度学习的课程
```



3. 为啥词向量 乘以一个矩阵 WQ 得出来的矩阵 就代表 查询？ 这个查询是词向量的特征吗？ 能举例说明一下 “我” 这个词向量的查询特殊是啥吗？

   ![image-20251129100955143](/Users/peilizhi/Library/Application Support/typora-user-images/image-20251129100955143.png)

4. 通过pytorch 包模拟一下transform 架构来实现 机器翻译 。单词的实际向量怎么展示的？



5. layer Normalization 与batch  Normalization 的区别![image-20251129114128615](https://raw.githubusercontent.com/Footman56/images/master/img202511291141684.png)

<img src="https://raw.githubusercontent.com/Footman56/images/master/img202511291142492.png" alt="image-20251129114209442" style="zoom:50%;" />

6. 解码器里面 K,V 的输入代表什么？ 在训练阶段 掩码器是怎么操作的？

7. 多层编码器与解码器的输入是怎么串联起来的？
8. 多头注意力机制里面的多头是什么意思？
