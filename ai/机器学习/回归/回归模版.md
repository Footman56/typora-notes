```python
# 引入依赖包
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import os
```

```python
# -------------------------
# User config / hyperparams
# -------------------------
DATA_PATH = "/kaggle/input/linear-regression-dataset/Linear Regression - Sheet1.csv"   # <-- 替换为你的数据路径，CSV 必须包含列 x 和 y
TEST_SIZE = 0.2
RANDOM_STATE = 42
BATCH_SIZE = 32 # 分批训练集大小
LR = 1e-2 # 步长
EPOCHS = 100
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
PRINT_EVERY = 10  # 每多少个 epoch 打印一次训练信息
MODEL_SAVE_PATH = "sigmoid_two_terms_model.pt" # 保存模型地址
```

```python

# 1) 读取数据并预处理：删除空行、检查长度并按 8:2 划分
df = pd.read_csv(DATA_PATH)
print("原始行数、列数：", df.shape)

# 删除含有任意空值的行（你之前要求）
df = df.dropna(axis=0)
print("删除空行后行数、列数：", df.shape)

# 检查是否存在 x,y 列
if "X" not in df.columns or "Y" not in df.columns:
    raise ValueError("CSV 必须包含列名 'x' 和 'y'。")
```

```python
# 选取 x, y 并转换为 numpy
x = df["X"].values.astype(np.float32)
y = df["Y"].values.astype(np.float32)
```

```python
# 划分测试集和训练集 8:2来划分测试集和训练集
X_train_np, X_val_np, y_train_np, y_val_np = train_test_split(
    x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE
)
```

```python
# 记录训练集的统计量以便在推理时使用相同变换
x_mean, x_std = X_train_np.mean(), X_train_np.std() if X_train_np.std() > 0 else 1.0
y_mean, y_std = y_train_np.mean(), y_train_np.std() if y_train_np.std() > 0 else 1.0

# 标准化（将 x,y 变为零均值、单位方差）——常能加速训练  
# 使用训练集的均值和标准差，来简单归一化（对回归通常有益）
X_train_np = (X_train_np - x_mean) / x_std
X_val_np = (X_val_np - x_mean) / x_std
y_train_np = (y_train_np - y_mean) / y_std
y_val_np = (y_val_np - y_mean) / y_std
```

```python
# 转为 torch tensors 并创建 DataLoader
X_train = torch.from_numpy(X_train_np).unsqueeze(1)  # shape (N,1)
X_val = torch.from_numpy(X_val_np).unsqueeze(1)
y_train = torch.from_numpy(y_train_np).unsqueeze(1)
y_val = torch.from_numpy(y_val_np).unsqueeze(1)

# 1.把训练数据和标签封装成 PyTorch 数据集 2. 让 DataLoader 能按批次读取 3.保证样本与标签的一一对应
train_ds = TensorDataset(X_train, y_train)
val_ds = TensorDataset(X_val, y_val)


# 1.将train_ds转换成可迭代的数据 2.可以批量读取数据，不需要整个读取 3.并且顺序打乱避免记住顺序
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)
```

```python
# -------------------------
# 2) 定义模型
# -------------------------
class TwoSigmoidModel(nn.Module):
    """
    模型： y_hat = b + c1 * sigmoid(b1 + w1 * x) + c2 * sigmoid(b2 + w2 * x)
    我们把所有这些量都设为可以学习的参数（nn.Parameter）。
    """
    def __init__(self):
        super().__init__()
        # 初始化参数为小随机值或零
        # b: 全局偏置 (scalar)
        self.b = nn.Parameter(torch.randn(1) * 0.1)

        # term1 parameters
        self.c1 = nn.Parameter(torch.randn(1) * 0.1)
        self.b1 = nn.Parameter(torch.randn(1) * 0.1)
        self.w1 = nn.Parameter(torch.randn(1) * 0.1)

        # term2 parameters
        self.c2 = nn.Parameter(torch.randn(1) * 0.1)
        self.b2 = nn.Parameter(torch.randn(1) * 0.1)
        self.w2 = nn.Parameter(torch.randn(1) * 0.1)

        self.sigmoid = torch.sigmoid

    def forward(self, x):
        # x shape: (batch, 1)
        term1 = self.c1 * self.sigmoid(self.b1 + self.w1 * x)
        term2 = self.c2 * self.sigmoid(self.b2 + self.w2 * x)
        return self.b + term1 + term2

```



```python
# 实例化并放到设备上
model = TwoSigmoidModel().to(DEVICE)

# -------------------------
# 3) 损失函数与优化器
# -------------------------
criterion = nn.MSELoss()  # 回归常用的均方误差 
# 创建优化器 optimizer，使用 Adam 优化算法。model.parameters() 会把模型中所有可学习参数传给优化器，lr=LR 指定学习率
optimizer = torch.optim.Adam(model.parameters(), lr=LR)  # Adam 通常收敛稳定


# -------------------------
# 4) 训练循环（含验证）
# -------------------------

# 初始化两个列表，用来记录每个 epoch 的训练损失和验证损失（便于画学习曲线或调参分析）。
train_losses = []
val_losses = []

for epoch in range(1, EPOCHS + 1):
    model.train() # 将model转换成训练模式，建议使用
    epoch_loss = 0.0
    # 每次遍历train_loader都是 一组张量
    for xb, yb in train_loader:
      	#重要：输入数据也必须 .to(DEVICE) 才能和模型在同一设备上进行计算，否则会报错
        xb = xb.to(DEVICE)
        yb = yb.to(DEVICE)

        preds = model(xb)          # 调用forward，根据模型计算预测值 preds
        loss = criterion(preds, yb)  # 计算损失
        optimizer.zero_grad()       # 清梯度 在反向传播之前把模型参数的梯度清零。PyTorch 中梯度是累加的（grad += ...），所以每次 backward() 前必须 zero_grad()，否则会把上一次梯度叠加进来。
        loss.backward()             # 反向传播计算梯度 计算损失对模型参数的梯度，并把这些梯度累加到参数的 .grad 属性上
        optimizer.step()            # 优化器根据参数的 .grad 值更新参数

        epoch_loss += loss.item() * xb.size(0)

    epoch_loss /= len(train_loader.dataset) # 将累加的总损失除以训练集样本总数，得到该 epoch 的平均训练损失（MSE per sample）
    train_losses.append(epoch_loss) 

    # 验证集评估（不计算梯度）
    model.eval() # 将model 切换到评估模式
    val_loss_sum = 0.0
    
    with torch.no_grad():  # 进入不计算梯度的模式：在其内部所有操作不会构建梯度计算图，也不会占用额外内存用于保存中间梯度。验证时必须使用它以节省内存并加速推理。
        for xb, yb in val_loader:
            xb = xb.to(DEVICE)
            yb = yb.to(DEVICE)
            preds = model(xb)   # 
            vloss = criterion(preds, yb)
            val_loss_sum += vloss.item() * xb.size(0)
    val_loss = val_loss_sum / len(val_loader.dataset)
    val_losses.append(val_loss)

    if epoch % PRINT_EVERY == 0 or epoch == 1 or epoch == EPOCHS:
        print(f"Epoch {epoch:4d}/{EPOCHS}  train_loss={epoch_loss:.6f}  val_loss={val_loss:.6f}")

```



把当前 batch 的损失累加到 `epoch_loss`：

- `loss.item()`：把只有在计算图里的标量张量转换为 Python float（取值），方便累加/打印。`nn.MSELoss()` 默认返回的是 batch 内元素的平均损失（`mean`），如果直接把每个 batch 的 `loss.item()` 相加再除以 batch 数，会因为最后一个 batch 的大小不同导致平均被轻微扭曲。乘以 batch 大小把它转成该 batch 的损失总和，再除以总样本数得到真正的样本平均损失。
- `xb.size(0)`：当前 batch 的样本数（最后一个 batch 可能小于 `batch_size`），这里乘以样本数的目的是把**按 batch 平均的损失**恢复为**该 batch 的总损失**（方便后面对整个数据集做加权平均）。
   举例：如果 `criterion` 返回 batch 内的均损 `mean_loss_batch`，那么 `mean_loss_batch * batch_size = sum_of_losses_for_batch`。
   最终我们会把 `epoch_loss` 除以 dataset 大小得到 epoch 的样本平均损失。